# ğŸ¤– AI KOC Support vá»›i Qwen2.5

Há»‡ thá»‘ng AI há»— trá»£ khÃ¡ch hÃ ng thÃ´ng minh sá»­ dá»¥ng **Qwen2.5-7B-Instruct** local model.

## âœ¨ TÃ­nh NÄƒng

### ğŸ§  AI ThÃ´ng Minh
- **Qwen2.5-7B-Instruct**: Model AI tiÃªn tiáº¿n cá»§a Alibaba
- **Local Processing**: Cháº¡y hoÃ n toÃ n offline, báº£o máº­t tuyá»‡t Ä‘á»‘i
- **Vietnamese Support**: Tá»‘i Æ°u cho tiáº¿ng Viá»‡t
- **Context Aware**: Hiá»ƒu ngá»¯ cáº£nh vÃ  tráº£ lá»i thÃ´ng minh

### ğŸš€ Performance
- **GPU Acceleration**: Tá»± Ä‘á»™ng detect CUDA
- **Lazy Loading**: Load model khi cáº§n thiáº¿t
- **Memory Efficient**: Tá»‘i Æ°u RAM vÃ  VRAM
- **Fallback System**: Backup responses khi model lá»—i

## ğŸ“‹ YÃªu Cáº§u Há»‡ Thá»‘ng

### ğŸ’» Hardware
- **RAM**: Tá»‘i thiá»ƒu 16GB (khuyáº¿n nghá»‹ 32GB)
- **GPU**: NVIDIA GPU vá»›i 8GB+ VRAM (tÃ¹y chá»n)
- **Storage**: 15GB+ cho model
- **CPU**: Multi-core processor

### ğŸ Software
- **Python**: 3.8+
- **CUDA**: 11.8+ (náº¿u dÃ¹ng GPU)
- **Git LFS**: Äá»ƒ download model

## ğŸ› ï¸ CÃ i Äáº·t

### 1. Clone Repository
```bash
git clone <your-repo>
cd AI-Support
```

### 2. CÃ i Dependencies
```bash
# CÃ i packages cáº§n thiáº¿t
pip install -r requirements_api.txt

# Náº¿u cÃ³ GPU NVIDIA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 3. Download Qwen2.5 Model

#### Option A: Tá»« HuggingFace (Recommended)
```bash
# CÃ i git-lfs náº¿u chÆ°a cÃ³
git lfs install

# Clone model
git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
```

#### Option B: Tá»« Local Path
```bash
# Náº¿u báº¡n Ä‘Ã£ cÃ³ model local
export QWEN_MODEL_PATH="/path/to/your/qwen2.5/model"
```

### 4. Cáº¥u HÃ¬nh Environment
```bash
# Táº¡o file .env
cp .env.example .env

# Chá»‰nh sá»­a path model
echo "QWEN_MODEL_PATH=./Qwen2.5-7B-Instruct" > .env
```

## ğŸš€ Cháº¡y Há»‡ Thá»‘ng

### 1. Start API Server
```bash
# Vá»›i auto-reload
python start_api_server.py --reload

# Production mode
python start_api_server.py --workers 4
```

### 2. Start Frontend
```bash
# Terminal má»›i
python start_frontend.py
```

### 3. Truy Cáº­p
- **Chat Interface**: http://localhost:3000
- **API Demo**: http://localhost:3000/demo.html
- **API Docs**: http://localhost:8000/docs

## ğŸ¯ CÃ¡ch Sá»­ Dá»¥ng

### ğŸ’¬ Chat Interface
1. Má»Ÿ http://localhost:3000
2. Nháº­p cÃ¢u há»i tiáº¿ng Viá»‡t
3. AI sáº½ tráº£ lá»i thÃ´ng minh vá»›i Qwen2.5

### ğŸ§ª Test API
```bash
curl -X POST "http://localhost:8000/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-assistant",
    "messages": [{"role": "user", "content": "Xin chÃ o"}],
    "stream": false
  }'
```

### ğŸ“Š Streaming Response
```javascript
const response = await fetch('/chat/completions', {
  method: 'POST',
  headers: {'Content-Type': 'application/json'},
  body: JSON.stringify({
    model: 'qwen2.5-assistant',
    messages: [{role: 'user', content: 'App cháº¡y cháº­m quÃ¡'}],
    stream: true
  })
});

// Process SSE stream
const reader = response.body.getReader();
// ... handle streaming
```

## âš™ï¸ Cáº¥u HÃ¬nh

### ğŸ›ï¸ Model Settings
```python
# Trong api/responses.py
QWEN_MODEL_PATH = "./Qwen2.5-7B-Instruct"  # Model path
DEVICE = "cuda"  # hoáº·c "cpu"

# Generation parameters
max_new_tokens=300,
temperature=0.7,
do_sample=True
```

### ğŸ”§ Performance Tuning
```python
# GPU Memory optimization
torch_dtype=torch.float16  # Cho GPU
torch_dtype=torch.float32  # Cho CPU

# Device mapping
device_map="auto"  # Tá»± Ä‘á»™ng phÃ¢n bá»• GPU
```

## ğŸ› Troubleshooting

### âŒ "CUDA out of memory"
```bash
# Giáº£m batch size hoáº·c dÃ¹ng CPU
export CUDA_VISIBLE_DEVICES=""  # Force CPU
```

### âŒ "Model not found"
```bash
# Kiá»ƒm tra path model
ls -la ./Qwen2.5-7B-Instruct/
# Hoáº·c download láº¡i
git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
```

### âŒ "Slow response"
```bash
# Kiá»ƒm tra GPU
nvidia-smi

# Hoáº·c dÃ¹ng smaller model
# Qwen2.5-3B-Instruct thay vÃ¬ 7B
```

## ğŸ“ˆ Performance Benchmarks

### ğŸ–¥ï¸ Hardware Specs
| Component | Recommended | Minimum |
|-----------|-------------|---------|
| GPU | RTX 4090 24GB | GTX 1080 8GB |
| RAM | 32GB | 16GB |
| CPU | 16 cores | 8 cores |

### âš¡ Response Times
| Setup | First Load | Subsequent |
|-------|------------|------------|
| GPU (RTX 4090) | ~10s | ~2-3s |
| GPU (RTX 3080) | ~15s | ~3-5s |
| CPU (16 cores) | ~30s | ~10-15s |

## ğŸ”„ Model Updates

### Cáº­p Nháº­t Qwen2.5
```bash
cd Qwen2.5-7B-Instruct
git pull origin main
```

### Thay Äá»•i Model
```bash
# Trong .env
QWEN_MODEL_PATH=./Qwen2.5-14B-Instruct  # Larger model
# hoáº·c
QWEN_MODEL_PATH=./Qwen2.5-3B-Instruct   # Smaller model
```

## ğŸ¨ Customization

### ğŸ¯ System Prompt
```python
# Chá»‰nh sá»­a trong api/responses.py
SYSTEM_PROMPT = """
Báº¡n lÃ  AI Assistant chuyÃªn nghiá»‡p...
[TÃ¹y chá»‰nh theo nhu cáº§u]
"""
```

### ğŸ”§ Generation Parameters
```python
# Fine-tune response quality
temperature=0.7,      # Creativity (0.1-1.0)
max_new_tokens=300,   # Response length
do_sample=True,       # Enable sampling
top_p=0.9,           # Nucleus sampling
```

## ğŸ“š API Documentation

### ğŸ”— Endpoints
- `GET /health` - Health check
- `GET /models` - Available models
- `POST /chat/completions` - Chat vá»›i AI
- `GET /stats` - System statistics

### ğŸ“ Request Format
```json
{
  "model": "qwen2.5-assistant",
  "messages": [
    {"role": "user", "content": "Your question"}
  ],
  "stream": true,
  "temperature": 0.7,
  "max_tokens": 300
}
```

## ğŸ¤ Contributing

1. Fork repository
2. Táº¡o feature branch
3. Commit changes
4. Push vÃ  táº¡o PR

## ğŸ“„ License

MIT License - Xem file LICENSE

---

## ğŸš€ Quick Start Commands

```bash
# 1. Setup
git clone <repo> && cd AI-Support
pip install -r requirements_api.txt
git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct

# 2. Run
python start_api_server.py --reload  # Terminal 1
python start_frontend.py             # Terminal 2

# 3. Test
curl -X POST localhost:8000/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen2.5","messages":[{"role":"user","content":"Xin chÃ o"}]}'
```

**Ready to chat with Qwen2.5! ğŸ¤–âœ¨** 