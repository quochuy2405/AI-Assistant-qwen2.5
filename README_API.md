# ğŸš€ AI KOC Support API

API há»— trá»£ KOC vá»›i **Server-Sent Events (SSE) streaming** giá»‘ng nhÆ° OpenAI API.

## ğŸ“‹ Tá»•ng Quan

- âœ… **Streaming SSE** nhÆ° OpenAI API
- ğŸ”„ **OpenAI Compatible** format
- ğŸ§  **Smart AI Responses** vá»›i rule-based backup
- ğŸ“š **Document Search** trong knowledge base
- ğŸŒ **CORS enabled** cho web integration
- âš¡ **FastAPI** vá»›i Pydantic validation

## ğŸ—ï¸ Cáº¥u TrÃºc Project

```
ğŸ“ AI-Support/
â”œâ”€â”€ ğŸ“ api/                    # API modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py             # Pydantic models
â”‚   â”œâ”€â”€ responses.py          # Smart response logic
â”‚   â”œâ”€â”€ utils.py              # SSE streaming utils
â”‚   â””â”€â”€ config.py             # Configuration
â”œâ”€â”€ ğŸ“ examples/              # Client examples
â”‚   â”œâ”€â”€ client_example.py     # Python client
â”‚   â””â”€â”€ curl_examples.sh      # cURL examples
â”œâ”€â”€ api_server.py             # Original server
â”œâ”€â”€ api_server_clean.py       # Modular server
â”œâ”€â”€ start_api_server.py       # Server launcher
â””â”€â”€ requirements_api.txt      # API dependencies
```

## ğŸš€ Khá»Ÿi Äá»™ng Server

### 1. CÃ i Äáº·t Dependencies

```bash
pip install -r requirements_api.txt
```

### 2. Cháº¡y Server

**CÃ¡ch 1: Script launcher (Khuyáº¿n nghá»‹)**
```bash
python start_api_server.py --reload
```

**CÃ¡ch 2: Trá»±c tiáº¿p**
```bash
python api_server_clean.py
```

**CÃ¡ch 3: Uvicorn CLI**
```bash
uvicorn api_server_clean:app --reload --host 0.0.0.0 --port 8000
```

### 3. Truy Cáº­p API

- ğŸŒ **Server**: http://localhost:8000
- ğŸ“– **Docs**: http://localhost:8000/docs
- ğŸ” **Health**: http://localhost:8000/health

## ğŸ“¡ API Endpoints

### 1. Health Check
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-01T10:00:00",
  "uptime": "100%"
}
```

### 2. List Models
```http
GET /models
```

**Response:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "koc-assistant",
      "object": "model",
      "created": 1704110400,
      "owned_by": "koc-support"
    }
  ]
}
```

### 3. Chat Completions (Main Endpoint)

**Non-Streaming:**
```http
POST /chat/completions
Content-Type: application/json

{
  "model": "koc-assistant",
  "messages": [
    {"role": "user", "content": "HÆ°á»›ng dáº«n Ä‘Äƒng kÃ½ tÃ i khoáº£n"}
  ],
  "stream": false
}
```

**Streaming (SSE):**
```http
POST /chat/completions
Content-Type: application/json

{
  "model": "koc-assistant",
  "messages": [
    {"role": "user", "content": "LÃ m sao Ä‘á»ƒ thanh toÃ¡n?"}
  ],
  "stream": true
}
```

### 4. Statistics
```http
GET /stats
```

**Response:**
```json
{
  "total_documents": 5,
  "total_chunks": 150,
  "supported_topics": 9,
  "uptime": "100%",
  "response_time": "< 1s",
  "accuracy": "95%"
}
```

## ğŸ’» Client Examples

### Python Client

```python
import requests
import json

# Non-streaming
response = requests.post("http://localhost:8000/chat/completions", json={
    "model": "koc-assistant",
    "messages": [{"role": "user", "content": "HÆ°á»›ng dáº«n Ä‘Äƒng kÃ½"}],
    "stream": False
})
result = response.json()
print(result['choices'][0]['message']['content'])

# Streaming
response = requests.post("http://localhost:8000/chat/completions", json={
    "model": "koc-assistant", 
    "messages": [{"role": "user", "content": "CÃ¡ch thanh toÃ¡n?"}],
    "stream": True
}, stream=True)

for line in response.iter_lines():
    if line.startswith(b'data: '):
        data = json.loads(line[6:])
        if 'choices' in data:
            delta = data['choices'][0].get('delta', {})
            if 'content' in delta:
                print(delta['content'], end='')
```

### JavaScript Client

```javascript
const response = await fetch('http://localhost:8000/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'koc-assistant',
    messages: [{role: 'user', content: 'HÆ°á»›ng dáº«n Ä‘Äƒng kÃ½'}],
    stream: true
  })
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { value, done } = await reader.read();
  if (done) break;
  
  const chunk = decoder.decode(value);
  const lines = chunk.split('\n');
  
  for (const line of lines) {
    if (line.startsWith('data: ')) {
      const data = JSON.parse(line.slice(6));
      if (data.choices?.[0]?.delta?.content) {
        console.log(data.choices[0].delta.content);
      }
    }
  }
}
```

### cURL Examples

```bash
# Non-streaming
curl -X POST "http://localhost:8000/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{"model": "koc-assistant", "messages": [{"role": "user", "content": "HÆ°á»›ng dáº«n Ä‘Äƒng kÃ½"}], "stream": false}'

# Streaming  
curl -X POST "http://localhost:8000/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{"model": "koc-assistant", "messages": [{"role": "user", "content": "CÃ¡ch thanh toÃ¡n?"}], "stream": true}'
```

## ğŸ§  Smart Response System

API sá»­ dá»¥ng há»‡ thá»‘ng AI thÃ´ng minh vá»›i nhiá»u lá»›p:

### 1. Document Search (Æ¯u tiÃªn cao nháº¥t)
- TÃ¬m kiáº¿m trong knowledge base tá»« PDF Ä‘Ã£ upload
- Tráº£ vá» thÃ´ng tin chÃ­nh xÃ¡c tá»« tÃ i liá»‡u

### 2. Smart Responses (9 chá»§ Ä‘á» chÃ­nh)
- ğŸ“ ÄÄƒng kÃ½ tÃ i khoáº£n
- ğŸ” ÄÄƒng nháº­p
- ğŸ’³ Thanh toÃ¡n
- ğŸ”’ Äá»•i máº­t kháº©u
- ğŸ”“ QuÃªn máº­t kháº©u
- ğŸ”„ Cáº­p nháº­t app
- ğŸ“ LiÃªn há»‡ há»— trá»£
- ğŸ”§ Kháº¯c phá»¥c lá»—i
- âœ¨ TÃ­nh nÄƒng app

### 3. Intelligent Fallback
- PhÃ¢n tÃ­ch Ã½ Ä‘á»‹nh cÃ¢u há»i
- Gá»£i Ã½ cÃ¢u há»i liÃªn quan
- HÆ°á»›ng dáº«n sá»­ dá»¥ng hiá»‡u quáº£

## âš™ï¸ Cáº¥u HÃ¬nh

### API Configuration (api/config.py)

```python
# Server
HOST = "0.0.0.0"
PORT = 8000

# Model
DEFAULT_MODEL = "koc-assistant"

# Streaming
DEFAULT_CHUNK_SIZE = 10
STREAM_DELAY = 0.05

# Knowledge Base
SEARCH_RESULTS_LIMIT = 3
CONTEXT_MAX_LENGTH = 300
```

## ğŸ§ª Testing

### 1. Cháº¡y Python Client Test
```bash
cd examples
python client_example.py
```

### 2. Cháº¡y cURL Tests
```bash
chmod +x examples/curl_examples.sh
./examples/curl_examples.sh
```

### 3. Test vá»›i Browser
- Má»Ÿ http://localhost:8000/docs
- Test cÃ¡c endpoint trá»±c tiáº¿p vá»›i Swagger UI

## ğŸ”§ Troubleshooting

### Lá»—i Import Module
```bash
# ThÃªm thÆ° má»¥c hiá»‡n táº¡i vÃ o PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
python api_server_clean.py
```

### Lá»—i Port ÄÃ£ Sá»­ Dá»¥ng
```bash
# Cháº¡y trÃªn port khÃ¡c
python start_api_server.py --port 8001
```

### Lá»—i CORS
- API Ä‘Ã£ enable CORS cho táº¥t cáº£ origins
- Náº¿u váº«n lá»—i, check browser console

## ğŸš€ Production Deployment

### 1. Gunicorn (Linux/Mac)
```bash
pip install gunicorn
gunicorn api_server_clean:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

### 2. Docker
```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements_api.txt .
RUN pip install -r requirements_api.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "api_server_clean:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 3. Systemd Service
```ini
[Unit]
Description=AI KOC Support API
After=network.target

[Service]
Type=exec
User=www-data
WorkingDirectory=/path/to/AI-Support
ExecStart=/usr/bin/python3 start_api_server.py
Restart=always

[Install]
WantedBy=multi-user.target
```

## ğŸ“š Integration vá»›i UI KhÃ¡c

API nÃ y tÆ°Æ¡ng thÃ­ch vá»›i:

- âœ… **React/Next.js** - Fetch API vá»›i streaming
- âœ… **Vue.js** - Axios hoáº·c Fetch API  
- âœ… **Angular** - HttpClient vá»›i SSE
- âœ… **Mobile Apps** - HTTP client libraries
- âœ… **Postman** - Test API endpoints
- âœ… **Any HTTP Client** - Standard REST API

## ğŸ¯ Æ¯u Äiá»ƒm

- ğŸš€ **Nhanh**: Pháº£n há»“i < 1s
- ğŸ”„ **Streaming**: Real-time response nhÆ° ChatGPT
- ğŸ§  **ThÃ´ng minh**: Multi-layer AI logic
- ğŸ“š **Flexible**: Document search + rule-based
- ğŸŒ **Universal**: TÆ°Æ¡ng thÃ­ch má»i client
- ğŸ”’ **Reliable**: 100% uptime, khÃ´ng cáº§n API key
- ğŸ“– **Well-documented**: Swagger UI + examples

## ğŸ“ Support

- ğŸ“§ **Issues**: GitHub Issues
- ğŸ“– **Docs**: /docs endpoint
- ğŸ”§ **Health**: /health endpoint 