import streamlit as st
import ollama
import tempfile
import os
from pathlib import Path
import json
from datetime import datetime

# Import cÃ¡c module tá»± táº¡o
from pdf_processor import PDFProcessor
from knowledge_base import KnowledgeBase
from ai_assistant import AIAssistant

# Prompt tá»‘i Æ°u cho Qwen2.5 vÃ  hÆ°á»›ng dáº«n tiáº¿ng Viá»‡t
OPTIMIZED_SYSTEM_PROMPT = """Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh chuyÃªn hÆ°á»›ng dáº«n sá»­ dá»¥ng á»©ng dá»¥ng. 

QUAN TRá»ŒNG: 
- LUÃ”N tráº£ lá»i báº±ng tiáº¿ng Viá»‡t
- ÄÆ°a ra hÆ°á»›ng dáº«n rÃµ rÃ ng, tá»«ng bÆ°á»›c
- Sá»­ dá»¥ng thÃ´ng tin tá»« tÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c táº£i lÃªn
- Náº¿u khÃ´ng cÃ³ thÃ´ng tin, hÃ£y nÃ³i "TÃ´i cáº§n thÃªm thÃ´ng tin Ä‘á»ƒ tráº£ lá»i chÃ­nh xÃ¡c"

Phong cÃ¡ch tráº£ lá»i:
- ThÃ¢n thiá»‡n vÃ  dá»… hiá»ƒu
- Chia thÃ nh cÃ¡c bÆ°á»›c cá»¥ thá»ƒ
- ÄÆ°a ra vÃ­ dá»¥ khi cáº§n thiáº¿t
- Sá»­ dá»¥ng emoji Ä‘á»ƒ lÃ m rÃµ rÃ ng hÆ¡n"""

def main():
    st.set_page_config(
        page_title="AI Há»— Trá»£ KOC - HÆ°á»›ng Dáº«n App",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ¤– AI Há»— Trá»£ KOC - HÆ°á»›ng Dáº«n App")
    st.markdown("**Sá»­ dá»¥ng LLAMA miá»…n phÃ­ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i vá» hÆ°á»›ng dáº«n app**")
    
    # Sidebar cho cáº¥u hÃ¬nh
    with st.sidebar:
        st.header("âš™ï¸ Cáº¥u HÃ¬nh")
        
        # Kiá»ƒm tra Ollama
        if check_ollama_connection():
            st.success("âœ… Ollama Ä‘Ã£ káº¿t ná»‘i")
        else:
            st.error("âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i Ollama")
            st.info("HÃ£y cÃ i Ä‘áº·t vÃ  cháº¡y Ollama trÆ°á»›c")
            return
        
        # Chá»n model
        available_models = get_available_models()
        if available_models:
            selected_model = st.selectbox(
                "Chá»n Model LLAMA:",
                available_models,
                index=0  # LuÃ´n chá»n model Ä‘áº§u tiÃªn trong danh sÃ¡ch
            )
            st.info(f"ğŸ¤– Äang sá»­ dá»¥ng: {selected_model}")
        else:
            st.error("KhÃ´ng tÃ¬m tháº¥y model nÃ o")
            return
    
    # Tab chÃ­nh
    tab1, tab2, tab3 = st.tabs(["ğŸ“š Táº£i TÃ i Liá»‡u", "ğŸ’¬ TrÃ² Chuyá»‡n", "ğŸ“Š Thá»‘ng KÃª"])
    
    with tab1:
        handle_document_upload(selected_model)
    
    with tab2:
        handle_chat_interface(selected_model)
    
    with tab3:
        handle_statistics()

def check_ollama_connection():
    """Kiá»ƒm tra káº¿t ná»‘i vá»›i Ollama"""
    try:
        response = ollama.list()
        return True
    except Exception:
        return False

def get_available_models():
    """Láº¥y danh sÃ¡ch models cÃ³ sáºµn"""
    try:
        response = ollama.list()
        models = []
        for model in response.models:
            if hasattr(model, 'name'):
                models.append(model.name)
            elif hasattr(model, 'model'):
                models.append(model.model)
            elif isinstance(model, dict):
                models.append(model.get('name', model.get('model', 'unknown')))
        
        # Æ¯u tiÃªn Qwen vÃ  cÃ¡c model tá»‘t
        qwen_models = [m for m in models if 'qwen' in m.lower()]
        llama_models = [m for m in models if 'llama' in m.lower()]
        other_models = [m for m in models if 'qwen' not in m.lower() and 'llama' not in m.lower()]
        
        return qwen_models + llama_models + other_models
        
    except Exception as e:
        print(f"Error getting models: {e}")  # Debug
        # Fallback vá»›i models Ä‘Ã£ cÃ i Ä‘áº·t
        return ["qwen2.5:7b", "llama2:7b-chat", "llama2:latest"]

def handle_document_upload(model_name):
    """Xá»­ lÃ½ upload vÃ  há»c tÃ i liá»‡u PDF"""
    st.header("ğŸ“š Táº£i TÃ i Liá»‡u HÆ°á»›ng Dáº«n")
    
    uploaded_files = st.file_uploader(
        "Chá»n file PDF hÆ°á»›ng dáº«n app:",
        type=['pdf'],
        accept_multiple_files=True,
        help="CÃ³ thá»ƒ táº£i lÃªn nhiá»u file PDF cÃ¹ng lÃºc"
    )
    
    if uploaded_files:
        st.success(f"ÄÃ£ chá»n {len(uploaded_files)} file(s)")
        
        if st.button("ğŸ”„ Xá»­ LÃ½ vÃ  Há»c TÃ i Liá»‡u", type="primary"):
            process_documents(uploaded_files, model_name)

def process_documents(uploaded_files, model_name):
    """Xá»­ lÃ½ vÃ  há»c cÃ¡c tÃ i liá»‡u PDF"""
    processor = PDFProcessor()
    knowledge_base = KnowledgeBase()
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_files = len(uploaded_files)
    
    for i, uploaded_file in enumerate(uploaded_files):
        status_text.text(f"Äang xá»­ lÃ½ {uploaded_file.name}...")
        
        # LÆ°u file táº¡m
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        
        try:
            # TrÃ­ch xuáº¥t text tá»« PDF
            text_chunks = processor.extract_and_chunk_pdf(tmp_path)
            
            # ThÃªm vÃ o knowledge base
            knowledge_base.add_documents(text_chunks, uploaded_file.name)
            
            progress_bar.progress((i + 1) / total_files)
            
        except Exception as e:
            st.error(f"Lá»—i xá»­ lÃ½ {uploaded_file.name}: {str(e)}")
        finally:
            # XÃ³a file táº¡m
            os.unlink(tmp_path)
    
    status_text.text("âœ… HoÃ n thÃ nh!")
    st.success("ÄÃ£ há»c xong táº¥t cáº£ tÃ i liá»‡u!")

def handle_chat_interface(model_name):
    """Xá»­ lÃ½ giao diá»‡n chat vá»›i KOC"""
    st.header("ğŸ’¬ TrÃ² Chuyá»‡n vá»›i AI Assistant")
    
    # Khá»Ÿi táº¡o chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Hiá»ƒn thá»‹ lá»‹ch sá»­ chat
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input cho cÃ¢u há»i má»›i
    if prompt := st.chat_input("Há»i vá» hÆ°á»›ng dáº«n app..."):
        # ThÃªm cÃ¢u há»i cá»§a user
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Táº¡o pháº£n há»“i tá»« AI
        with st.chat_message("assistant"):
            response = get_ai_response(prompt, model_name)
            st.markdown(response)
            
        # ThÃªm pháº£n há»“i vÃ o lá»‹ch sá»­
        st.session_state.messages.append({"role": "assistant", "content": response})

def get_ai_response(question, model_name):
    """Láº¥y pháº£n há»“i tá»« AI vá»›i Qwen2.5 tá»‘i Æ°u hÃ³a"""
    
    # Context tá»« tÃ i liá»‡u
    context_docs = KnowledgeBase().search(question, k=5)
    
    # Táº¡o prompt optimized cho Qwen
    messages = [
        {
            "role": "system", 
            "content": OPTIMIZED_SYSTEM_PROMPT
        }
    ]
    
    # ThÃªm lá»‹ch sá»­ há»™i thoáº¡i (giá»›i háº¡n 2 lÆ°á»£t gáº§n nháº¥t)
    conversation_history = st.session_state.messages[-4:] if "messages" in st.session_state else []
    for msg in conversation_history:
        messages.append(msg)
    
    # Context message
    if context_docs:
        context = "\n".join([doc['content'] for doc in context_docs])
        messages.append({
            "role": "system",
            "content": f"ThÃ´ng tin tham kháº£o tá»« tÃ i liá»‡u:\n{context}"
        })
    
    # CÃ¢u há»i ngÆ°á»i dÃ¹ng
    messages.append({
        "role": "user",
        "content": question
    })
    
    try:
        # Tá»‘i Æ°u hÃ³a cho Qwen2.5
        response = ollama.chat(
            model=model_name,
            messages=messages,
            options={
                "temperature": 0.3,
                "top_p": 0.9,
                "num_predict": 400,
                "repeat_penalty": 1.1
            }
        )
        
        if response:
            answer = response.get('message', {}).get('content', '')
            
            # Äáº£m báº£o luÃ´n cÃ³ tiáº¿ng Viá»‡t
            if answer and not any(ord(char) > 127 for char in answer[:50]):
                answer = "Xin chÃ o! ğŸ˜Š " + answer
                
            return answer
        else:
            return "âŒ Lá»—i káº¿t ná»‘i vá»›i AI. Vui lÃ²ng thá»­ láº¡i."
            
    except Exception as e:
        return f"âŒ CÃ³ lá»—i xáº£y ra: {str(e)}"

def handle_statistics():
    """Hiá»ƒn thá»‹ thá»‘ng kÃª sá»­ dá»¥ng"""
    st.header("ğŸ“Š Thá»‘ng KÃª Sá»­ Dá»¥ng")
    
    knowledge_base = KnowledgeBase()
    stats = knowledge_base.get_statistics()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Sá»‘ tÃ i liá»‡u", stats.get('total_documents', 0))
    
    with col2:
        st.metric("Sá»‘ Ä‘oáº¡n text", stats.get('total_chunks', 0))
    
    with col3:
        st.metric("Sá»‘ cÃ¢u há»i Ä‘Ã£ tráº£ lá»i", len(st.session_state.get('messages', [])) // 2)
    
    # Hiá»ƒn thá»‹ danh sÃ¡ch tÃ i liá»‡u
    if stats.get('documents'):
        st.subheader("ğŸ“‹ Danh SÃ¡ch TÃ i Liá»‡u")
        for doc in stats['documents']:
            st.text(f"â€¢ {doc}")

if __name__ == "__main__":
    main() 