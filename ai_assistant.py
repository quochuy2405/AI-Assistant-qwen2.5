import ollama
from typing import List, Dict, Optional
import json
import time
from datetime import datetime

class AIAssistant:
    """AI Assistant sá»­ dá»¥ng LLAMA Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i KOC"""
    
    def __init__(self, model_name: str = "llama2:latest"):
        self.model_name = model_name
        self.conversation_history = []
        
        # Template prompt cho viá»‡c tráº£ lá»i cÃ¢u há»i
        self.system_prompt = """AI há»— trá»£ ngÆ°á»i Viá»‡t. 

QUAN TRá»ŒNG: Chá»‰ tráº£ lá»i báº±ng tiáº¿ng Viá»‡t. Cáº¥m tiáº¿ng Anh.

ðŸŽ¯ NHIá»†M Vá»¤: Tráº£ lá»i ngáº¯n gá»n, thÃ¢n thiá»‡n vá» hÆ°á»›ng dáº«n app."""
    
    def generate_response(self, question: str, context: str = "", conversation_history: List[Dict] = None) -> str:
        """
        Táº¡o pháº£n há»“i cho cÃ¢u há»i cá»§a KOC
        
        Args:
            question: CÃ¢u há»i cá»§a KOC
            context: Ngá»¯ cáº£nh tá»« tÃ i liá»‡u liÃªn quan
            conversation_history: Lá»‹ch sá»­ há»™i thoáº¡i
            
        Returns:
            Pháº£n há»“i cá»§a AI
        """
        try:
            # XÃ¢y dá»±ng prompt
            prompt = self._build_prompt(question, context, conversation_history)
            
            # Gá»i LLAMA
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompt + "\n\nQUAN TRá»ŒNG: Tráº£ lá»i hoÃ n toÃ n báº±ng TIáº¾NG VIá»†T. KhÃ´ng Ä‘Æ°á»£c dÃ¹ng tiáº¿ng Anh."},
                    {"role": "user", "content": prompt},
                    {"role": "assistant", "content": "Xin chÃ o! ðŸ˜Š "}  # Force start in Vietnamese
                ],
                options={
                    "temperature": 0.3,      # Giáº£m tá»« 0.7 Ä‘á»ƒ nhanh hÆ¡n
                    "top_p": 0.8,           # Giáº£m tá»« 0.9
                    "max_tokens": 300,      # Giáº£m tá»« 1024 Ä‘á»ƒ nhanh hÆ¡n
                    "num_predict": 300,     # Limit prediction
                    "repeat_penalty": 1.1,  # TrÃ¡nh láº·p láº¡i
                    "stop": ["English:", "In English", "[English]"],
                }
            )
            
            # Láº¥y ná»™i dung pháº£n há»“i
            answer = "Xin chÃ o! ðŸ˜Š " + response['message']['content'].strip()
            
            # LÆ°u vÃ o lá»‹ch sá»­
            self.conversation_history.append({
                "question": question,
                "answer": answer,
                "context_used": bool(context),
                "timestamp": datetime.now().isoformat()
            })
            
            return answer
            
        except Exception as e:
            return f"Xin lá»—i, tÃ´i gáº·p lá»—i khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n: {str(e)}"
    
    def _build_prompt(self, question: str, context: str, conversation_history: List[Dict] = None) -> str:
        """XÃ¢y dá»±ng prompt ngáº¯n gá»n cho LLAMA"""
        
        prompt_parts = []
        
        # ThÃªm context náº¿u cÃ³
        if context:
            prompt_parts.append("TÃ€I LIá»†U:")
            prompt_parts.append(context[:500] + "...")  # Giá»›i háº¡n context
            prompt_parts.append("")
        
        # CÃ¢u há»i
        prompt_parts.append("CÃ‚U Há»ŽI:")
        prompt_parts.append(question)
        prompt_parts.append("")
        
        # HÆ°á»›ng dáº«n ngáº¯n
        prompt_parts.append("Tráº£ lá»i ngáº¯n gá»n báº±ng tiáº¿ng Viá»‡t:")
        
        return "\n".join(prompt_parts)
    
    def generate_follow_up_questions(self, question: str, answer: str) -> List[str]:
        """Táº¡o cÃ¢u há»i tiáº¿p theo cÃ³ thá»ƒ há»¯u Ã­ch"""
        try:
            prompt = f"""
Dá»±a trÃªn cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i sau, hÃ£y Ä‘á» xuáº¥t 3 cÃ¢u há»i tiáº¿p theo mÃ  KOC cÃ³ thá»ƒ quan tÃ¢m:

CÃ¢u há»i gá»‘c: {question}
CÃ¢u tráº£ lá»i: {answer}

Äá» xuáº¥t 3 cÃ¢u há»i tiáº¿p theo (má»—i cÃ¢u má»™t dÃ²ng, báº¯t Ä‘áº§u báº±ng dáº¥u -):
"""
            
            response = ollama.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                options={"temperature": 0.8, "max_tokens": 200}
            )
            
            # Xá»­ lÃ½ pháº£n há»“i
            content = response['message']['content'].strip()
            questions = []
            
            for line in content.split('\n'):
                line = line.strip()
                if line.startswith('-'):
                    questions.append(line[1:].strip())
                elif line and not line.startswith('Äá» xuáº¥t') and not line.startswith('CÃ¢u há»i'):
                    questions.append(line)
            
            return questions[:3]  # Chá»‰ láº¥y tá»‘i Ä‘a 3 cÃ¢u
            
        except Exception:
            return []
    
    def summarize_conversation(self, messages: List[Dict]) -> str:
        """TÃ³m táº¯t cuá»™c há»™i thoáº¡i"""
        try:
            # Táº¡o text tá»« lá»‹ch sá»­ há»™i thoáº¡i
            conversation_text = ""
            for msg in messages:
                role = "KOC" if msg['role'] == 'user' else "AI"
                conversation_text += f"{role}: {msg['content']}\n"
            
            prompt = f"""
HÃ£y tÃ³m táº¯t cuá»™c há»™i thoáº¡i sau giá»¯a KOC vÃ  AI Assistant vá» hÆ°á»›ng dáº«n app:

{conversation_text}

TÃ³m táº¯t (2-3 cÃ¢u ngáº¯n gá»n):
"""
            
            response = ollama.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                options={"temperature": 0.5, "max_tokens": 150}
            )
            
            return response['message']['content'].strip()
            
        except Exception:
            return "KhÃ´ng thá»ƒ tÃ³m táº¯t cuá»™c há»™i thoáº¡i."
    
    def check_question_relevance(self, question: str) -> Dict[str, any]:
        """Kiá»ƒm tra cÃ¢u há»i cÃ³ liÃªn quan Ä‘áº¿n hÆ°á»›ng dáº«n app khÃ´ng"""
        try:
            prompt = f"""
CÃ¢u há»i: {question}

HÃ£y Ä‘Ã¡nh giÃ¡ cÃ¢u há»i nÃ y:
1. CÃ³ liÃªn quan Ä‘áº¿n hÆ°á»›ng dáº«n sá»­ dá»¥ng app khÃ´ng? (cÃ³/khÃ´ng)
2. Loáº¡i cÃ¢u há»i (hÆ°á»›ng dáº«n/troubleshooting/tÃ­nh nÄƒng/khÃ¡c)
3. Má»©c Ä‘á»™ cá»¥ thá»ƒ (cao/trung bÃ¬nh/tháº¥p)

Tráº£ lá»i báº±ng JSON format:
{{"relevant": true/false, "type": "...", "specificity": "..."}}
"""
            
            response = ollama.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                options={"temperature": 0.3, "max_tokens": 100}
            )
            
            content = response['message']['content'].strip()
            
            # Thá»­ parse JSON
            try:
                return json.loads(content)
            except:
                # Fallback
                return {
                    "relevant": True,
                    "type": "general",
                    "specificity": "medium"
                }
                
        except Exception:
            return {
                "relevant": True,
                "type": "unknown",
                "specificity": "unknown"
            }
    
    def get_conversation_stats(self) -> Dict[str, any]:
        """Láº¥y thá»‘ng kÃª cuá»™c há»™i thoáº¡i"""
        if not self.conversation_history:
            return {
                "total_questions": 0,
                "avg_response_time": 0,
                "context_usage": 0
            }
        
        total_questions = len(self.conversation_history)
        questions_with_context = sum(1 for item in self.conversation_history if item.get('context_used', False))
        
        return {
            "total_questions": total_questions,
            "context_usage_rate": round(questions_with_context / total_questions * 100, 1) if total_questions > 0 else 0,
            "last_question_time": self.conversation_history[-1]['timestamp'] if self.conversation_history else None
        }
    
    def clear_history(self):
        """XÃ³a lá»‹ch sá»­ há»™i thoáº¡i"""
        self.conversation_history = []
    
    def set_model(self, model_name: str):
        """Thay Ä‘á»•i model LLAMA"""
        self.model_name = model_name 