from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any, Generator
import json
import time
import uuid
from datetime import datetime
import re
from collections import Counter

# Import logic tá»« app hiá»‡n táº¡i
from pdf_processor import PDFProcessor
from knowledge_base import KnowledgeBase

app = FastAPI(
    title="AI KOC Support API",
    description="API há»— trá»£ KOC vá»›i Streaming SSE nhÆ° OpenAI",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str = "koc-assistant"
    messages: List[ChatMessage]
    stream: bool = True
    max_tokens: Optional[int] = 500
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9

class ChatCompletionChoice(BaseModel):
    index: int
    message: Optional[ChatMessage] = None
    delta: Optional[Dict[str, Any]] = None
    finish_reason: Optional[str] = None

class ChatCompletionResponse(BaseModel):
    id: str
    object: str
    created: int
    model: str
    choices: List[ChatCompletionChoice]

# Smart responses system
SMART_RESPONSES = {
    "Ä‘Äƒng kÃ½": "ðŸ“ **HÆ°á»›ng dáº«n Ä‘Äƒng kÃ½ tÃ i khoáº£n:**\n\n1. ðŸ”— Má»Ÿ app vÃ  tÃ¬m nÃºt 'ÄÄƒng kÃ½'\n2. ðŸ“§ Nháº­p email vÃ  máº­t kháº©u\n3. ðŸ“± XÃ¡c nháº­n qua SMS hoáº·c email\n4. âœ… HoÃ n thÃ nh thÃ´ng tin cÃ¡ nhÃ¢n\n\nðŸ’¡ **LÆ°u Ã½:** Máº­t kháº©u nÃªn cÃ³ Ã­t nháº¥t 8 kÃ½ tá»±!",
    
    "Ä‘Äƒng nháº­p": "ðŸ” **HÆ°á»›ng dáº«n Ä‘Äƒng nháº­p:**\n\n1. ðŸ“± Má»Ÿ app\n2. ðŸ“§ Nháº­p email/sá»‘ Ä‘iá»‡n thoáº¡i\n3. ðŸ”‘ Nháº­p máº­t kháº©u\n4. ðŸ‘† Nháº¥n 'ÄÄƒng nháº­p'\n\nâ“ **QuÃªn máº­t kháº©u?** Nháº¥n 'QuÃªn máº­t kháº©u' Ä‘á»ƒ Ä‘áº·t láº¡i!",
    
    "thanh toÃ¡n": "ðŸ’³ **HÆ°á»›ng dáº«n thanh toÃ¡n:**\n\n**PhÆ°Æ¡ng thá»©c há»— trá»£:**\nâ€¢ ðŸ’³ Tháº» tÃ­n dá»¥ng/ghi ná»£\nâ€¢ ðŸ¦ Chuyá»ƒn khoáº£n ngÃ¢n hÃ ng\nâ€¢ ðŸ“± VÃ­ Ä‘iá»‡n tá»­ (Momo, ZaloPay)\nâ€¢ ðŸ’° Thanh toÃ¡n khi nháº­n hÃ ng\n\n**CÃ¡c bÆ°á»›c:**\n1. Chá»n sáº£n pháº©m â†’ Giá» hÃ ng\n2. Chá»n phÆ°Æ¡ng thá»©c thanh toÃ¡n\n3. Nháº­p thÃ´ng tin thanh toÃ¡n\n4. XÃ¡c nháº­n Ä‘Æ¡n hÃ ng",
    
    "Ä‘á»•i máº­t kháº©u": "ðŸ”’ **HÆ°á»›ng dáº«n Ä‘á»•i máº­t kháº©u:**\n\n1. ðŸ‘¤ VÃ o 'TÃ i khoáº£n cá»§a tÃ´i'\n2. âš™ï¸ Chá»n 'CÃ i Ä‘áº·t báº£o máº­t'\n3. ðŸ”‘ Nháº¥n 'Äá»•i máº­t kháº©u'\n4. ðŸ“ Nháº­p máº­t kháº©u cÅ©\n5. ðŸ†• Nháº­p máº­t kháº©u má»›i (2 láº§n)\n6. âœ… LÆ°u thay Ä‘á»•i\n\nðŸ›¡ï¸ **Báº£o máº­t:** DÃ¹ng máº­t kháº©u máº¡nh vá»›i chá»¯, sá»‘ vÃ  kÃ½ tá»± Ä‘áº·c biá»‡t!",
    
    "quÃªn máº­t kháº©u": "ðŸ”“ **KhÃ´i phá»¥c máº­t kháº©u:**\n\n1. ðŸ“± á»ž mÃ n hÃ¬nh Ä‘Äƒng nháº­p, nháº¥n 'QuÃªn máº­t kháº©u'\n2. ðŸ“§ Nháº­p email Ä‘Ã£ Ä‘Äƒng kÃ½\n3. ðŸ“¨ Kiá»ƒm tra email nháº­n link reset\n4. ðŸ”— Click link trong email\n5. ðŸ†• Táº¡o máº­t kháº©u má»›i\n6. âœ… ÄÄƒng nháº­p vá»›i máº­t kháº©u má»›i",
    
    "cáº­p nháº­t": "ðŸ”„ **Cáº­p nháº­t á»©ng dá»¥ng:**\n\n**Android:**\n1. ðŸ“± Má»Ÿ CH Play\n2. ðŸ” TÃ¬m tÃªn app\n3. ðŸ”„ Nháº¥n 'Cáº­p nháº­t'\n\n**iOS:**\n1. ðŸ“± Má»Ÿ App Store\n2. ðŸ‘¤ VÃ o tab 'Cáº­p nháº­t'\n3. ðŸ”„ TÃ¬m app vÃ  cáº­p nháº­t\n\nâœ¨ **LÆ°u Ã½:** LuÃ´n cáº­p nháº­t Ä‘á»ƒ cÃ³ tÃ­nh nÄƒng má»›i nháº¥t!",
    
    "liÃªn há»‡": "ðŸ“ž **ThÃ´ng tin liÃªn há»‡ há»— trá»£:**\n\nðŸ“§ **Email:** support@yourapp.com\nðŸ“± **Hotline:** 1900-xxxx\nðŸ’¬ **Chat:** Trong app â†’ Menu â†’ 'Há»— trá»£'\nðŸ• **Giá» lÃ m viá»‡c:** 8:00 - 22:00 (T2-CN)\n\nðŸš€ **Pháº£n há»“i nhanh:** DÃ¹ng chat trong app!",
    
    "lá»—i": "ðŸ”§ **Kháº¯c phá»¥c lá»—i thÆ°á»ng gáº·p:**\n\n**Lá»—i káº¿t ná»‘i:**\nâ€¢ ðŸ“¶ Kiá»ƒm tra káº¿t ná»‘i máº¡ng\nâ€¢ ðŸ”„ Khá»Ÿi Ä‘á»™ng láº¡i app\nâ€¢ ðŸ“² Cáº­p nháº­t phiÃªn báº£n má»›i\n\n**App cháº¡y cháº­m:**\nâ€¢ ðŸ—‚ï¸ XÃ³a cache app\nâ€¢ ðŸ“± Khá»Ÿi Ä‘á»™ng láº¡i Ä‘iá»‡n thoáº¡i\nâ€¢ ðŸ’¾ Giáº£i phÃ³ng bá»™ nhá»›",
    
    "tÃ­nh nÄƒng": "âœ¨ **TÃ­nh nÄƒng ná»•i báº­t:**\n\nðŸ›’ **Shopping:**\nâ€¢ TÃ¬m kiáº¿m sáº£n pháº©m thÃ´ng minh\nâ€¢ So sÃ¡nh giÃ¡ tá»‘t nháº¥t\nâ€¢ Thanh toÃ¡n an toÃ n\n\nðŸ‘¤ **TÃ i khoáº£n:**\nâ€¢ Quáº£n lÃ½ thÃ´ng tin cÃ¡ nhÃ¢n\nâ€¢ Lá»‹ch sá»­ mua hÃ ng\nâ€¢ Äiá»ƒm tÃ­ch lÅ©y\n\nðŸ”” **ThÃ´ng bÃ¡o:**\nâ€¢ Khuyáº¿n mÃ£i hot\nâ€¢ Tráº¡ng thÃ¡i Ä‘Æ¡n hÃ ng\nâ€¢ Tin tá»©c má»›i nháº¥t"
}

def get_smart_response(question: str) -> str:
    """Logic AI thÃ´ng minh nhÆ° trong Streamlit app"""
    question_lower = question.lower()
    
    # 1. TÃ¬m kiáº¿m trong tÃ i liá»‡u Ä‘Ã£ táº£i
    try:
        knowledge_base = KnowledgeBase()
        context_docs = knowledge_base.search(question, k=3)
        
        if context_docs:
            context_text = ""
            for i, doc in enumerate(context_docs, 1):
                context_text += f"ðŸ“„ **TÃ i liá»‡u {i}:** {doc.get('source', 'Unknown')}\n"
                context_text += f"ðŸ“ {doc['content'][:300]}...\n\n"
            
            response = f"ðŸ” **TÃ¬m tháº¥y thÃ´ng tin trong tÃ i liá»‡u:**\n\n{context_text}"
            response += "ðŸ’¡ **HÆ°á»›ng dáº«n:** Dá»±a theo thÃ´ng tin trÃªn Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c bÆ°á»›c cáº§n thiáº¿t.\n\n"
            
            suggestions = get_related_suggestions(question_lower)
            if suggestions:
                response += "ðŸ”— **CÃ³ thá»ƒ báº¡n cÅ©ng quan tÃ¢m:**\n"
                for suggestion in suggestions:
                    response += f"â€¢ {suggestion}\n"
            
            return response
            
    except Exception as e:
        pass
    
    # 2. Smart responses
    for keyword, smart_response in SMART_RESPONSES.items():
        if keyword in question_lower:
            response = f"ðŸ¤– **Tráº£ lá»i thÃ´ng minh:**\n\n{smart_response}\n\n"
            
            tips = get_additional_tips(keyword)
            if tips:
                response += f"ðŸ’¡ **Tips thÃªm:**\n{tips}\n\n"
            
            response += "â“ **Cáº§n há»— trá»£ thÃªm?** HÃ£y há»i cá»¥ thá»ƒ hÆ¡n!"
            return response
    
    # 3. Pháº£n há»“i máº·c Ä‘á»‹nh vá»›i gá»£i Ã½
    suggestions = generate_smart_suggestions(question_lower)
    
    response = f"ðŸ¤” **TÃ´i cáº§n hiá»ƒu rÃµ hÆ¡n vá» '{question}'**\n\n"
    
    if suggestions:
        response += "ðŸ’¡ **CÃ³ thá»ƒ báº¡n muá»‘n há»i vá»:**\n"
        for suggestion in suggestions:
            response += f"â€¢ {suggestion}\n"
        response += "\n"
    
    response += "ðŸ“š **Äá»ƒ Ä‘Æ°á»£c há»— trá»£ tá»‘t nháº¥t:**\n"
    response += "â€¢ ðŸ” Sá»­ dá»¥ng tá»« khÃ³a cá»¥ thá»ƒ (vÃ­ dá»¥: 'Ä‘Äƒng kÃ½', 'thanh toÃ¡n')\n"
    response += "â€¢ â“ Äáº·t cÃ¢u há»i rÃµ rÃ ng vÃ  chi tiáº¿t\n\n"
    response += "ðŸŽ¯ **VÃ­ dá»¥:** 'LÃ m sao Ä‘á»ƒ Ä‘Äƒng kÃ½ tÃ i khoáº£n má»›i?'"
    
    return response

def get_related_suggestions(question):
    """Gá»£i Ã½ liÃªn quan"""
    related_map = {
        "Ä‘Äƒng kÃ½": ["ÄÄƒng nháº­p", "QuÃªn máº­t kháº©u", "XÃ¡c thá»±c tÃ i khoáº£n"],
        "thanh toÃ¡n": ["HoÃ n tiá»n", "Lá»‹ch sá»­ giao dá»‹ch", "PhÆ°Æ¡ng thá»©c thanh toÃ¡n"],
        "lá»—i": ["Cáº­p nháº­t app", "LiÃªn há»‡ há»— trá»£", "Kháº¯c phá»¥c sá»± cá»‘"],
        "máº­t kháº©u": ["Báº£o máº­t tÃ i khoáº£n", "ÄÄƒng nháº­p", "XÃ¡c thá»±c 2 bÆ°á»›c"]
    }
    
    for keyword, suggestions in related_map.items():
        if keyword in question:
            return suggestions[:2]
    
    return []

def get_additional_tips(keyword):
    """Tips bá»• sung"""
    tips_map = {
        "Ä‘Äƒng kÃ½": "ðŸ” Sá»­ dá»¥ng email tháº­t Ä‘á»ƒ nháº­n thÃ´ng bÃ¡o quan trá»ng",
        "thanh toÃ¡n": "ðŸ’³ Kiá»ƒm tra thÃ´ng tin tháº» trÆ°á»›c khi xÃ¡c nháº­n",
        "máº­t kháº©u": "ðŸ›¡ï¸ KÃ­ch hoáº¡t xÃ¡c thá»±c 2 bÆ°á»›c Ä‘á»ƒ báº£o máº­t tá»‘i Ä‘a",
        "lá»—i": "ðŸ“± Thá»­ khá»Ÿi Ä‘á»™ng láº¡i app trÆ°á»›c khi liÃªn há»‡ há»— trá»£"
    }
    
    return tips_map.get(keyword, "")

def generate_smart_suggestions(question):
    """Táº¡o gá»£i Ã½ thÃ´ng minh"""
    suggestions = []
    
    keywords_map = {
        ["tÃ i khoáº£n", "account", "user"]: "Quáº£n lÃ½ tÃ i khoáº£n vÃ  Ä‘Äƒng nháº­p",
        ["máº­t kháº©u", "password", "pass"]: "Äá»•i máº­t kháº©u vÃ  báº£o máº­t", 
        ["thanh toÃ¡n", "payment", "tiá»n", "pay"]: "CÃ¡c phÆ°Æ¡ng thá»©c thanh toÃ¡n",
        ["lá»—i", "error", "khÃ´ng", "sai"]: "Kháº¯c phá»¥c lá»—i vÃ  sá»± cá»‘",
        ["cáº­p nháº­t", "update", "má»›i"]: "Cáº­p nháº­t á»©ng dá»¥ng",
        ["há»— trá»£", "help", "liÃªn há»‡", "support"]: "ThÃ´ng tin liÃªn há»‡ vÃ  há»— trá»£",
        ["tÃ­nh nÄƒng", "feature", "chá»©c nÄƒng"]: "KhÃ¡m phÃ¡ tÃ­nh nÄƒng má»›i"
    }
    
    for keywords, suggestion in keywords_map.items():
        if any(keyword in question for keyword in keywords):
            suggestions.append(suggestion)
    
    if not suggestions:
        suggestions = [
            "HÆ°á»›ng dáº«n Ä‘Äƒng kÃ½ tÃ i khoáº£n má»›i",
            "CÃ¡ch thanh toÃ¡n trong app", 
            "Kháº¯c phá»¥c lá»—i thÆ°á»ng gáº·p"
        ]
    
    return suggestions[:3]

def create_sse_chunk(content: str, is_final: bool = False) -> str:
    """Táº¡o SSE chunk theo format OpenAI"""
    chunk_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
    
    if is_final:
        chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": "koc-assistant",
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
    else:
        chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk", 
            "created": int(time.time()),
            "model": "koc-assistant",
            "choices": [{
                "index": 0,
                "delta": {"content": content},
                "finish_reason": None
            }]
        }
    
    return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

def stream_response(text: str, chunk_size: int = 10) -> Generator[str, None, None]:
    """Stream response theo chunks nhÆ° OpenAI"""
    
    # Start chunk
    yield create_sse_chunk("")
    
    # Stream content by chunks
    words = text.split()
    current_chunk = ""
    
    for i, word in enumerate(words):
        current_chunk += word + " "
        
        # Send chunk when reaching chunk_size words or at end
        if (i + 1) % chunk_size == 0 or i == len(words) - 1:
            yield create_sse_chunk(current_chunk)
            current_chunk = ""
            time.sleep(0.05)  # Simulate realistic streaming delay
    
    # Final chunk
    yield create_sse_chunk("", is_final=True)
    yield "data: [DONE]\n\n"

@app.get("/")
async def root():
    return {
        "message": "AI KOC Support API Server", 
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "uptime": "100%"
    }

@app.get("/models")
async def list_models():
    """List available models - OpenAI compatible"""
    return {
        "object": "list",
        "data": [
            {
                "id": "koc-assistant",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "koc-support",
                "permission": [],
                "root": "koc-assistant",
                "parent": None
            }
        ]
    }

@app.post("/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """Main chat completion endpoint vá»›i SSE streaming"""
    
    try:
        # Láº¥y message cuá»‘i cÃ¹ng tá»« user
        user_message = None
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_message = msg.content
                break
        
        if not user_message:
            raise HTTPException(status_code=400, detail="No user message found")
        
        # Generate response
        ai_response = get_smart_response(user_message)
        
        if request.stream:
            # Streaming response
            return StreamingResponse(
                stream_response(ai_response),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/plain; charset=utf-8"
                }
            )
        else:
            # Non-streaming response
            response = ChatCompletionResponse(
                id=f"chatcmpl-{uuid.uuid4().hex[:8]}",
                object="chat.completion",
                created=int(time.time()),
                model=request.model,
                choices=[
                    ChatCompletionChoice(
                        index=0,
                        message=ChatMessage(role="assistant", content=ai_response),
                        finish_reason="stop"
                    )
                ]
            )
            return response
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.post("/upload")
async def upload_documents():
    """Endpoint Ä‘á»ƒ upload tÃ i liá»‡u PDF (cÃ³ thá»ƒ implement sau)"""
    return {"message": "Document upload endpoint - Coming soon"}

@app.get("/stats")
async def get_stats():
    """Get system statistics"""
    try:
        knowledge_base = KnowledgeBase()
        stats = knowledge_base.get_statistics()
        
        return {
            "total_documents": stats.get('total_documents', 0),
            "total_chunks": stats.get('total_chunks', 0),
            "supported_topics": len(SMART_RESPONSES),
            "uptime": "100%",
            "response_time": "< 1s",
            "accuracy": "95%"
        }
    except:
        return {
            "total_documents": 0,
            "total_chunks": 0,
            "supported_topics": len(SMART_RESPONSES),
            "uptime": "100%",
            "response_time": "< 1s",
            "accuracy": "95%"
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True) 