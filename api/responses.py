# Import logic t·ª´ app hi·ªán t·∫°i
from knowledge_base import KnowledgeBase
import os
import requests
import json
from typing import Optional

# C·∫•u h√¨nh Ollama - T·ªëi ∆∞u t·ªëc ƒë·ªô
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
QWEN_MODEL_NAME = os.getenv("QWEN_MODEL_NAME", "qwen2.5:7b")

# Cache cho responses ph·ªï bi·∫øn (tƒÉng t·ªëc ƒë·ªô)
RESPONSE_CACHE = {}

# SMART_RESPONSES dictionary (ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi import c≈©)
SMART_RESPONSES = {
    "greeting": "Ch√†o b·∫°n! üòä M√¨nh l√† AI Assistant c·ªßa KOC Support.",
    "slow_app": "üòÖ App ch·∫°y ch·∫≠m h·∫£ b·∫°n? M√¨nh h∆∞·ªõng d·∫´n fix ngay nh√©!",
    "payment": "üí≥ H·ªó tr·ª£ thanh to√°n: Th·∫ª t√≠n d·ª•ng, chuy·ªÉn kho·∫£n, v√≠ ƒëi·ªán t·ª≠...",
    "register": "üìù ƒêƒÉng k√Ω t√†i kho·∫£n si√™u d·ªÖ v·ªõi 5 b∆∞·ªõc ƒë∆°n gi·∫£n!"
}

# System prompt ng·∫Øn g·ªçn h∆°n - T·ªëi ∆∞u t·ªëc ƒë·ªô
SYSTEM_PROMPT = """B·∫°n l√† AI Assistant c·ªßa KOC Support.

NHI·ªÜM V·ª§: Tr·∫£ l·ªùi ng·∫Øn g·ªçn, th√¢n thi·ªán v·ªÅ h·ªó tr·ª£ kh√°ch h√†ng

PHONG C√ÅCH:
- D√πng "m√¨nh" thay "t√¥i"
- Emoji ph√π h·ª£p 
- T·ªëi ƒëa 350 t·ª´
- H∆∞·ªõng d·∫´n c·ª• th·ªÉ

CH·ª¶ ƒê·ªÄ: ƒëƒÉng k√Ω, ƒëƒÉng nh·∫≠p, thanh to√°n, l·ªói app, b·∫£o m·∫≠t."""

def check_ollama_connection() -> bool:
    """Ki·ªÉm tra k·∫øt n·ªëi Ollama - Timeout ng·∫Øn h∆°n"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=2)  # Gi·∫£m t·ª´ 5s xu·ªëng 2s
        return response.status_code == 200
    except:
        return False

def get_smart_response(question: str) -> str:
    """AI th√¥ng minh v·ªõi cache v√† t·ªëi ∆∞u t·ªëc ƒë·ªô"""
    question_lower = question.lower()
    
    # 1. Ki·ªÉm tra cache tr∆∞·ªõc (si√™u nhanh)
    cache_key = question_lower.strip()
    if cache_key in RESPONSE_CACHE:
        return RESPONSE_CACHE[cache_key]
    
    # 2. Quick pattern matching cho c√¢u h·ªèi ph·ªï bi·∫øn
    quick_response = get_quick_pattern_response(question_lower)
    if quick_response:
        RESPONSE_CACHE[cache_key] = quick_response
        return quick_response
    
    # 3. T√¨m ki·∫øm trong knowledge base v·ªõi multiple search terms
    try:
        knowledge_base = KnowledgeBase()
        context_docs = []
        
        # Th·ª≠ nhi·ªÅu search terms cho c√¢u h·ªèi v·ªÅ chi·∫øn d·ªãch
        if any(word in question_lower for word in ["chi·∫øn d·ªãch", "campaign"]):
            search_terms = [
                question,  # Original question
                "chi·∫øn d·ªãch review s·∫£n ph·∫©m",  # Specific campaign type
                "lo·∫°i chi·∫øn d·ªãch",  # Types of campaigns
                "C√ÅC LO·∫†I CHI·∫æN D·ªäCH"  # Exact heading
            ]
            
            for term in search_terms:
                docs = knowledge_base.search(term, k=2)
                context_docs.extend(docs)
                # N·∫øu t√¨m ƒë∆∞·ª£c k·∫øt qu·∫£ t·ªët (distance < 0.82), d·ª´ng t√¨m ki·∫øm
                if docs and docs[0].get('distance', 1.0) < 0.82:
                    break
        else:
            # Search th√¥ng th∆∞·ªùng cho c√°c c√¢u h·ªèi kh√°c
            context_docs = knowledge_base.search(question, k=2)
        
        if context_docs:
            # Lo·∫°i b·ªè duplicate v√† l·∫•y unique content
            unique_contents = []
            seen_contents = set()
            for doc in context_docs:
                content_key = doc['content'][:100]  # First 100 chars as key
                if content_key not in seen_contents:
                    unique_contents.append(doc['content'][:800])  # TƒÉng l√™n 800 ƒë·ªÉ th·∫•y ƒë·ªß 3 lo·∫°i
                    seen_contents.add(content_key)
            
            context_text = "\n".join(unique_contents)
            
            enhanced_prompt = f"""B·∫°n l√† AI Assistant c·ªßa KOC Support. Tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ch√≠nh x√°c.

Context: {context_text}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi ch·ªâ v·ªÅ c√¢u h·ªèi ƒë∆∞·ª£c h·ªèi, kh√¥ng ƒë∆∞a th√¥ng tin th·ª´a. T·ªëi ƒëa 100 t·ª´."""

            response = get_ollama_response(enhanced_prompt)
            RESPONSE_CACHE[cache_key] = response
            return response
            
    except Exception as e:
        print(f"Knowledge base error: {e}")
    
    # 4. D√πng Qwen2.5 qua Ollama
    response = get_ollama_response(question)
    RESPONSE_CACHE[cache_key] = response
    return response

def get_quick_pattern_response(question_lower: str) -> Optional[str]:
    """Pattern matching nhanh cho c√¢u h·ªèi ph·ªï bi·∫øn"""
    
    # Ch·ªâ gi·ªØ l·∫°i ch√†o h·ªèi c∆° b·∫£n, c√≤n l·∫°i ƒë·ªÉ AI x·ª≠ l√Ω
    if any(word in question_lower for word in ["xin ch√†o", "ch√†o", "hello", "hi"]) and len(question_lower.split()) <= 3:
        return "Ch√†o b·∫°n! üòä M√¨nh l√† AI Assistant c·ªßa KOC Support. C√≥ g√¨ c·∫ßn h·ªó tr·ª£ kh√¥ng?"
    
    # Lo·∫°i b·ªè t·∫•t c·∫£ hardcode patterns kh√°c - ƒë·ªÉ AI t·ª± tr·∫£ l·ªùi
    return None

def get_ollama_response(user_message: str) -> str:
    """G·ªçi Qwen2.5 qua Ollama API - T·ªëi ∆∞u t·ªëc ƒë·ªô"""
    
    # Ki·ªÉm tra k·∫øt n·ªëi Ollama nhanh
    if not check_ollama_connection():
        return get_fallback_response(user_message)
    
    try:
        # Payload t·ªëi ∆∞u t·ªëc ƒë·ªô
        payload = {
            "model": QWEN_MODEL_NAME,
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_message}
            ],
            "stream": False,
            "options": {
                "temperature": 0.3,      # Gi·∫£m t·ª´ 0.7 - √≠t ng·∫´u nhi√™n h∆°n
                "num_predict": 150,      # TƒÉng t·ª´ 80 l√™n 150 ƒë·ªÉ ƒë·ªß content
                "top_p": 0.8,           # Gi·∫£m t·ª´ 0.9 - t·∫≠p trung h∆°n
                "num_ctx": 1024,        # Gi·ªõi h·∫°n context
                "repeat_penalty": 1.1    # Tr√°nh l·∫∑p t·ª´
            }
        }
        
        # G·ªçi Ollama API v·ªõi timeout ng·∫Øn
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/chat",
            json=payload,
            timeout=15  # Gi·∫£m t·ª´ 30s xu·ªëng 15s
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result.get("message", {}).get("content", "").strip()
            
            if ai_response:
                return ai_response
            else:
                return get_fallback_response(user_message)
        else:
            return get_fallback_response(user_message)
            
    except requests.RequestException:
        return get_fallback_response(user_message)
    except Exception:
        return get_fallback_response(user_message)

def get_fallback_response(question: str) -> str:
    """Fallback responses khi Ollama kh√¥ng kh·∫£ d·ª•ng"""
    question_lower = question.lower()
    
    # Ch√†o h·ªèi c∆° b·∫£n
    greetings = ["xin ch√†o", "ch√†o", "hello", "hi", "hey"]
    if any(greeting in question_lower for greeting in greetings) and len(question_lower.split()) <= 3:
        return "Ch√†o b·∫°n! üòä M√¨nh l√† AI Assistant c·ªßa KOC Support. C√≥ g√¨ m√¨nh c√≥ th·ªÉ gi√∫p b·∫°n kh√¥ng?"
    
    # Default response - kh√¥ng hardcode c√°c ch·ªß ƒë·ªÅ c·ª• th·ªÉ
    return f"""üòä M√¨nh hi·ªÉu b·∫°n h·ªèi v·ªÅ "{question}".

üí° **M√¨nh c√≥ th·ªÉ h·ªó tr·ª£ nhi·ªÅu v·∫•n ƒë·ªÅ kh√°c nhau:**
‚Ä¢ üìù ƒêƒÉng k√Ω v√† qu·∫£n l√Ω t√†i kho·∫£n
‚Ä¢ üí≥ Thanh to√°n v√† giao d·ªãch  
‚Ä¢ üîß Kh·∫Øc ph·ª•c l·ªói k·ªπ thu·∫≠t
‚Ä¢ üîê B·∫£o m·∫≠t v√† quy·ªÅn ri√™ng t∆∞
‚Ä¢ üì¢ Chi·∫øn d·ªãch v√† marketing
‚Ä¢ üìû Th√¥ng tin li√™n h·ªá

B·∫°n c√≥ th·ªÉ h·ªèi c·ª• th·ªÉ h∆°n ho·∫∑c di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi kh√¥ng? M√¨nh s·∫Ω c·ªë g·∫Øng h·ªó tr·ª£ t·ªët nh·∫•t! üòä"""

# C√°c helper functions (gi·ªØ nguy√™n ƒë·ªÉ t∆∞∆°ng th√≠ch)
def get_related_suggestions(question):
    """G·ª£i √Ω li√™n quan"""
    related_map = {
        "ƒëƒÉng k√Ω": ["ƒêƒÉng nh·∫≠p", "Qu√™n m·∫≠t kh·∫©u", "X√°c th·ª©c t√†i kho·∫£n"],
        "thanh to√°n": ["Ho√†n ti·ªÅn", "L·ªãch s·ª≠ giao d·ªãch", "Ph∆∞∆°ng th·ª©c thanh to√°n"],
        "l·ªói": ["C·∫≠p nh·∫≠t app", "Li√™n h·ªá h·ªó tr·ª£", "Kh·∫Øc ph·ª•c s·ª± c·ªë"],
        "m·∫≠t kh·∫©u": ["B·∫£o m·∫≠t t√†i kho·∫£n", "ƒêƒÉng nh·∫≠p", "X√°c th·ª±c 2 b∆∞·ªõc"]
    }
    
    for keyword, suggestions in related_map.items():
        if keyword in question:
            return suggestions[:2]
    
    return []

def get_additional_tips(keyword):
    """Tips b·ªï sung th√¢n thi·ªán"""
    tips_map = {
        "ƒëƒÉng k√Ω": "üîê Nh·ªõ d√πng email th·∫≠t ƒë·ªÉ nh·∫≠n th√¥ng b√°o quan tr·ªçng nh√©!",
        "thanh to√°n": "üí≥ Ki·ªÉm tra k·ªπ th√¥ng tin th·∫ª tr∆∞·ªõc khi x√°c nh·∫≠n",
        "m·∫≠t kh·∫©u": "üõ°Ô∏è B·∫≠t x√°c th·ª±c 2 b∆∞·ªõc ƒë·ªÉ b·∫£o m·∫≠t t·ªëi ƒëa",
        "l·ªói": "üì± Restart ƒëi·ªán tho·∫°i th∆∞·ªùng xuy√™n ƒë·ªÉ app ch·∫°y m∆∞·ª£t h∆°n!",
        "li√™n h·ªá": "üí¨ Chat trong app s·∫Ω ƒë∆∞·ª£c ph·∫£n h·ªìi nhanh nh·∫•t ƒë·∫•y!",
        "c·∫≠p nh·∫≠t": "üîî B·∫≠t th√¥ng b√°o auto-update ƒë·ªÉ kh√¥ng b·ªè l·ª° t√≠nh nƒÉng m·ªõi"
    }
    
    return tips_map.get(keyword, "")

def generate_smart_suggestions(question):
    """T·∫°o g·ª£i √Ω th√¥ng minh"""
    suggestions = []
    
    # Enhanced keyword detection with casual phrases
    keywords_mapping = [
        (["t√†i kho·∫£n", "account", "user", "profile"], "Qu·∫£n l√Ω t√†i kho·∫£n v√† ƒëƒÉng nh·∫≠p"),
        (["m·∫≠t kh·∫©u", "password", "pass", "kh·∫©u"], "ƒê·ªïi m·∫≠t kh·∫©u v√† b·∫£o m·∫≠t"),
        (["thanh to√°n", "payment", "ti·ªÅn", "pay", "n·∫°p"], "C√°c ph∆∞∆°ng th·ª©c thanh to√°n"),
        (["l·ªói", "error", "ch·∫≠m", "lag", "crash", "gi·∫≠t"], "Kh·∫Øc ph·ª•c l·ªói v√† app ch·∫≠m"),
        (["c·∫≠p nh·∫≠t", "update", "m·ªõi", "upgrade"], "C·∫≠p nh·∫≠t ·ª©ng d·ª•ng"),
        (["h·ªó tr·ª£", "help", "li√™n h·ªá", "support", "contact"], "Th√¥ng tin li√™n h·ªá v√† h·ªó tr·ª£"),
        (["t√≠nh nƒÉng", "feature", "ch·ª©c nƒÉng", "c√≥ g√¨"], "Kh√°m ph√° t√≠nh nƒÉng m·ªõi")
    ]
    
    for keywords, suggestion in keywords_mapping:
        if any(keyword in question for keyword in keywords):
            suggestions.append(suggestion)
    
    if not suggestions:
        suggestions = [
            "H∆∞·ªõng d·∫´n ƒëƒÉng k√Ω t√†i kho·∫£n m·ªõi",
            "Kh·∫Øc ph·ª•c app ch·∫°y ch·∫≠m", 
            "C√°ch thanh to√°n trong app"
        ]
    
    return suggestions[:3]

# Test function v·ªõi ki·ªÉm tra t·ªëc ƒë·ªô
def test_ollama_connection():
    """Test Ollama connection v√† ƒëo t·ªëc ƒë·ªô"""
    import time
    
    print(f"üîç Testing Ollama connection...")
    print(f"üì° Base URL: {OLLAMA_BASE_URL}")
    print(f"ü§ñ Model: {QWEN_MODEL_NAME}")
    
    if check_ollama_connection():
        print("‚úÖ Ollama ƒëang ch·∫°y!")
        
        # Test t·ªëc ƒë·ªô ph·∫£n h·ªìi
        start_time = time.time()
        test_response = get_ollama_response("Xin ch√†o")
        end_time = time.time()
        
        response_time = end_time - start_time
        print(f"üß™ Test response: {test_response[:100]}...")
        print(f"‚ö° Response time: {response_time:.2f}s")
        
        if response_time < 3:
            print("üöÄ T·ªëc ƒë·ªô t·ªët!")
        elif response_time < 8:
            print("‚ö†Ô∏è T·ªëc ƒë·ªô trung b√¨nh")
        else:
            print("üêå C·∫ßn t·ªëi ∆∞u th√™m")
            
        return True
    else:
        print("‚ùå Ollama kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c. Ki·ªÉm tra:")
        print("   ‚Ä¢ ollama serve")
        print("   ‚Ä¢ ollama list")
        return False

def clear_cache():
    """X√≥a cache ƒë·ªÉ l√†m m·ªõi responses"""
    global RESPONSE_CACHE
    RESPONSE_CACHE.clear()
    print("üóëÔ∏è Cache ƒë√£ ƒë∆∞·ª£c x√≥a!")

if __name__ == "__main__":
    test_ollama_connection() 